{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IamAVB/Style_transfer/blob/master/Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1tTAAzzrWoH7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a Stle transfer program which will transfer the artistic style of one image(style image) to another image(content image). \n",
        "\n",
        "This implementation is based on A Neural Algorithm of Artistic Style (Gatys et al., 2016) paper.\n",
        "\n",
        "Here we need  to import two images. One for taking the style from the  image and another for the image to be transformed. \n",
        "\n",
        "End results are stored in outputs folder which can be seen on the left panel. \n",
        "\n",
        "I have stored the output images for fewer steps because the changes between the successive images will be minimal. \n",
        "\n",
        "This program I wrote as part of stanford course work I was taking as a self study course. "
      ]
    },
    {
      "metadata": {
        "id": "QGwmNEckUl3D",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8bf577e2-674c-4c4a-accd-4baff52912b5"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de3c38b2-525f-4d96-8e36-164e5adca9c1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-de3c38b2-525f-4d96-8e36-164e5adca9c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Varagina Banakar Aravind_DACS.jpg to Varagina Banakar Aravind_DACS (1).jpg\n",
            "User uploaded file \"Varagina Banakar Aravind_DACS.jpg\" with length 5314 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v8XJHbYSYB8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will make use of pretrained object detection model VGG-19 weights as initial weights for our model. This will give better results over initializing the weights with normal randomized values."
      ]
    },
    {
      "metadata": {
        "id": "_K4tct6cZE_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import necessary packages that we are going to make use of.\n",
        "\n",
        "import os, time, scipy.misc\n",
        "import numpy as np # for data manupulations\n",
        "import scipy.io    \n",
        "import tensorflow as tf # Tensorflow as background. \n",
        "from PIL import Image, ImageOps # used for resizing the original images\n",
        "from six.moves import urllib    # Used to download VGG-19 model layer weights from URL. \n",
        "\n",
        "# Set TF_CPP_MIN_LOG_LEVEL to 0 All logs are shown (default)\n",
        "#                             1 to filter out INFO logs, \n",
        "#                             2 to additionall filter out WARNING, \n",
        "#                             3 to additionally filter out ERROR.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T43TOdXRTv3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to downlaod pretrained object detection VGG-19 model.\n",
        "# We also check the byte size once downloaded to verify whether downloaded flie is correct and no byte loss happened. \n",
        "def download(download_link, file_name, expected_bytes):\n",
        "    \"\"\" Download the pretrained VGG-19 model if it's not already downloaded \"\"\"\n",
        "    if os.path.exists(file_name):\n",
        "        print(\"Pre-trained Object detection VGG-19 model exists already.\")\n",
        "        return\n",
        "    print(\"Downloading the Pre-trained Object detection VGG-19 model. Wait till it's done\")\n",
        "    file_name, _ = urllib.request.urlretrieve(download_link, file_name)\n",
        "    file_stat = os.stat(file_name)\n",
        "    if file_stat.st_size == expected_bytes:\n",
        "        print('Successfully downloaded VGG-19 model', file_name)\n",
        "    else:\n",
        "        raise Exception('File ' + file_name +\n",
        "                        ' byte miss match. Files might be corrupted. Try downloading it from a browser.')\n",
        "\n",
        "# To resize content image and style image to match the dimension that we want(output image dimension).\n",
        "def get_resized_image(img_path, width, height, save=True):\n",
        "    image = Image.open(img_path)\n",
        "    # swap the places of width and height since it's column major.\n",
        "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
        "    if save:\n",
        "        image_dirs = img_path.split('/')\n",
        "        image_dirs[-1] = 'resized_' + image_dirs[-1]\n",
        "        out_path = '/'.join(image_dirs)\n",
        "        if not os.path.exists(out_path):\n",
        "            image.save(out_path)\n",
        "    image = np.asarray(image, np.float32)\n",
        "    return np.expand_dims(image, 0)\n",
        "\n",
        "def generate_noise_image(content_image, width, height, noise_ratio=0.6):\n",
        "    noise_image = np.random.uniform(-20, 20, (1, height, width, 3)).astype(np.float32)\n",
        "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
        "\n",
        "def save_image(path, image):\n",
        "    image = image[0]\n",
        "    image = np.clip(image, 0, 255).astype('uint8')\n",
        "    scipy.misc.imsave(path, image)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "      os.mkdir(path)\n",
        "    except OSError:\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPnwlR78TjZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VGG-19 parameters file\n",
        "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
        "VGG_FILENAME = 'imagenet-vgg-verydeep-19.mat'\n",
        "EXPECTED_BYTES = 534904783\n",
        "\n",
        "class VGG(object):\n",
        "    def __init__(self, input_img):\n",
        "        download(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
        "        self.vgg_layers = scipy.io.loadmat(VGG_FILENAME)['layers']\n",
        "        self.input_img = input_img\n",
        "        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) # mean pixel values got from VGG 19 mean centered values.\n",
        "\n",
        "    def _weights(self, layer_idx, expected_layer_name):\n",
        "        \"\"\" Return the weights and biases at layer_idx already trained by VGG\n",
        "        \"\"\"\n",
        "        W = self.vgg_layers[0][layer_idx][0][0][2][0][0]\n",
        "        b = self.vgg_layers[0][layer_idx][0][0][2][0][1]\n",
        "        layer_name = self.vgg_layers[0][layer_idx][0][0][0][0]\n",
        "        assert layer_name == expected_layer_name\n",
        "        return W, b.reshape(b.size)\n",
        "\n",
        "    def conv2d_relu(self, prev_layer, layer_idx, layer_name):\n",
        "        \"\"\" Return the Conv2D layer with RELU using the weights, biases from the VGG model at 'layer_idx'.\n",
        "        Inputs:\n",
        "            prev_layer: the output tensor from the previous layer\n",
        "            layer_idx: the index to current layer in vgg_layers\n",
        "            layer_name: the string that is the name of the current layer. It's used for variable_scope.\n",
        "        Note that you first need to obtain W and b from from the corresponding VGG's layer \n",
        "        using the function _weights() defined above.\n",
        "        W and b returned from _weights() are numpy arrays, so you have\n",
        "        to convert them to TF tensors. One way to do it is with tf.constant.\n",
        "        I am going to use SAME padding with stride of 1 since image is small. this can be modified to observe different outcome.\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        with tf.variable_scope(layer_name) as scope:\n",
        "            W, b = self._weights(layer_idx, layer_name)\n",
        "            W = tf.constant(W, name='weights')\n",
        "            b = tf.constant(b, name='bias')\n",
        "            conv2d = tf.nn.conv2d(prev_layer, \n",
        "                                filter=W, \n",
        "                                strides=[1, 1, 1, 1], \n",
        "                                padding='SAME')\n",
        "            out = tf.nn.relu(conv2d + b)\n",
        "        ###############################\n",
        "        setattr(self, layer_name, out)\n",
        "\n",
        "    def avgpool(self, prev_layer, layer_name):\n",
        "        \"\"\" Return the average pooling layer. The paper suggests that \n",
        "        average pooling works better than max pooling.\n",
        "        Input:\n",
        "            prev_layer: the output tensor from the previous layer\n",
        "            layer_name: the string that you want to name the layer.\n",
        "                        It's used to specify variable_scope.\n",
        "        # Kernel size and strides are chosen based on two three trials of different values.\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        with tf.variable_scope(layer_name):\n",
        "            out = tf.nn.avg_pool(prev_layer, \n",
        "                                ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "        ###############################\n",
        "        setattr(self, layer_name, out)\n",
        "\n",
        "    def load(self):\n",
        "        self.conv2d_relu(self.input_img, 0, 'conv1_1')\n",
        "        self.conv2d_relu(self.conv1_1, 2, 'conv1_2')\n",
        "        self.avgpool(self.conv1_2, 'avgpool1')\n",
        "        \n",
        "        self.conv2d_relu(self.avgpool1, 5, 'conv2_1')\n",
        "        self.conv2d_relu(self.conv2_1, 7, 'conv2_2')\n",
        "        self.avgpool(self.conv2_2, 'avgpool2')\n",
        "        \n",
        "        self.conv2d_relu(self.avgpool2, 10, 'conv3_1')\n",
        "        self.conv2d_relu(self.conv3_1, 12, 'conv3_2')\n",
        "        self.conv2d_relu(self.conv3_2, 14, 'conv3_3')\n",
        "        self.conv2d_relu(self.conv3_3, 16, 'conv3_4')\n",
        "        self.avgpool(self.conv3_4, 'avgpool3')\n",
        "        \n",
        "        self.conv2d_relu(self.avgpool3, 19, 'conv4_1')\n",
        "        self.conv2d_relu(self.conv4_1, 21, 'conv4_2')\n",
        "        self.conv2d_relu(self.conv4_2, 23, 'conv4_3')\n",
        "        self.conv2d_relu(self.conv4_3, 25, 'conv4_4')\n",
        "        self.avgpool(self.conv4_4, 'avgpool4')\n",
        "        \n",
        "        self.conv2d_relu(self.avgpool4, 28, 'conv5_1')\n",
        "        self.conv2d_relu(self.conv5_1, 30, 'conv5_2')\n",
        "        self.conv2d_relu(self.conv5_2, 32, 'conv5_3')\n",
        "        self.conv2d_relu(self.conv5_3, 34, 'conv5_4')\n",
        "        self.avgpool(self.conv5_4, 'avgpool5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PuPv9wyITNGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1707
        },
        "outputId": "5ff020e6-1971-4384-ccda-d93770e7f7f9"
      },
      "cell_type": "code",
      "source": [
        "def setup():\n",
        "    safe_mkdir('checkpoints') # check points will be saved in this folder \n",
        "    safe_mkdir('outputs')     # outputs folder.\n",
        "\n",
        "class StyleTransfer(object):\n",
        "    def __init__(self, content_img, style_img, img_width, img_height):\n",
        "        '''\n",
        "        img_width and img_height are the dimensions we expect from the generated image.\n",
        "        We will resize input content image and input style image to match this dimension using get_resize_image.\n",
        "        '''\n",
        "        self.img_width   = img_width\n",
        "        self.img_height  = img_height\n",
        "        self.content_img = get_resized_image(content_img, img_width, img_height)\n",
        "        self.style_img   = get_resized_image(style_img, img_width, img_height)\n",
        "        self.initial_img = generate_noise_image(self.content_img, img_width, img_height)\n",
        "\n",
        "        ###############################\n",
        "        ## created global step (gstep) and hyperparameters for the model\n",
        "        self.content_layer = 'conv4_2'\n",
        "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
        "        self.content_w = 0.01\n",
        "        self.style_w = 1\n",
        "        self.style_layer_w = [0.5, 1.0, 1.5, 3.0, 4.0] \n",
        "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
        "                                trainable=False, name='global_step')\n",
        "        self.learning_rate = 2.0\n",
        "        ###############################\n",
        "\n",
        "    def create_input(self):\n",
        "        '''\n",
        "        We will use one input_img as a placeholder for the content image, \n",
        "        style image, and generated image, because:\n",
        "            1. they have the same dimension\n",
        "            2. we have to extract the same set of features from them\n",
        "        We use a variable instead of a placeholder because we're, at the same time, \n",
        "        training the generated image to get the desirable result.\n",
        "        Note: image height corresponds to number of rows, not columns.\n",
        "        '''\n",
        "        with tf.variable_scope('input') as scope:\n",
        "            self.input_img = tf.get_variable('in_img', \n",
        "                                        shape=([1, self.img_height, self.img_width, 3]),\n",
        "                                        dtype=tf.float32,\n",
        "                                        initializer=tf.zeros_initializer())\n",
        "    def load_vgg(self):\n",
        "        '''\n",
        "        Load the saved model parameters of VGG-19, using the input_img\n",
        "        as the input to compute the output at each layer of vgg.\n",
        "        During training, VGG-19 mean-centered all images and found the mean pixels\n",
        "        to be [123.68, 116.779, 103.939] along RGB dimensions. We have to subtract\n",
        "        this mean from our images.\n",
        "        '''\n",
        "        self.vgg = VGG(self.input_img)\n",
        "        self.vgg.load()\n",
        "        self.content_img -= self.vgg.mean_pixels\n",
        "        self.style_img -= self.vgg.mean_pixels\n",
        "\n",
        "    def _content_loss(self, P, F):\n",
        "        ''' Calculate the loss between the feature representation of the\n",
        "        content image and the generated image.\n",
        "        \n",
        "        Inputs: \n",
        "            P: content representation of the content image\n",
        "            F: content representation of the generated image\n",
        "            Read the assignment handout for more details\n",
        "            Note: Don't use the coefficient 0.5 as defined in the paper.\n",
        "            Use the coefficient defined in the assignment handout.\n",
        "        '''\n",
        "        # There are two losses here. one is content loss and another is style loss. we need to minimize both.\n",
        "        # Here content loss is reduced based on formula mentioned in the paper.\n",
        "        ###############################\n",
        "        self.content_loss = tf.reduce_sum((F - P) ** 2) / (4.0 * P.size)\n",
        "        ###############################\n",
        "    \n",
        "    def _gram_matrix(self, F, N, M):\n",
        "        \"\"\" Create and return the gram matrix for tensor F\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        F = tf.reshape(F, (M, N))\n",
        "        return tf.matmul(tf.transpose(F), F)\n",
        "        ###############################\n",
        "\n",
        "    def _single_style_loss(self, a, g):\n",
        "        \"\"\" Calculate the style loss at a certain layer\n",
        "        Inputs:\n",
        "            a is the feature representation of the style image at that layer\n",
        "            g is the feature representation of the generated image at that layer\n",
        "        Output:\n",
        "            the style loss at a certain layer (which is E_l in the paper)\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        N = a.shape[3] # number of filters\n",
        "        M = a.shape[1] * a.shape[2] # height times width of the feature map\n",
        "        A = self._gram_matrix(a, N, M)\n",
        "        G = self._gram_matrix(g, N, M)\n",
        "        return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))\n",
        "        ###############################\n",
        "\n",
        "    def _style_loss(self, A):\n",
        "        \"\"\" The total style loss is a weighted sum of style losses at all style layers.\n",
        "        \"\"\"\n",
        "        n_layers = len(A)\n",
        "        #         _single_style_loss(  a ,     g)  \n",
        "        E = [self._single_style_loss(A[i], getattr(self.vgg, self.style_layers[i])) for i in range(n_layers)]\n",
        "        \n",
        "        ###############################\n",
        "        self.style_loss = sum([self.style_layer_w[i] * E[i] for i in range(n_layers)])\n",
        "        ###############################\n",
        "\n",
        "    def losses(self):\n",
        "        with tf.variable_scope('losses') as scope:\n",
        "            with tf.Session() as sess:\n",
        "                sess.run(self.input_img.assign(self.content_img)) \n",
        "                gen_img_content = getattr(self.vgg, self.content_layer)\n",
        "                content_img_content = sess.run(gen_img_content)\n",
        "            self._content_loss(content_img_content, gen_img_content)\n",
        "\n",
        "            with tf.Session() as sess:\n",
        "                sess.run(self.input_img.assign(self.style_img))\n",
        "                style_layers = sess.run([getattr(self.vgg, layer) for layer in self.style_layers])                              \n",
        "            self._style_loss(style_layers)\n",
        "\n",
        "            ##########################################\n",
        "            self.total_loss = self.content_w * self.content_loss + self.style_w * self.style_loss\n",
        "            ##########################################\n",
        "\n",
        "    def optimize(self):\n",
        "        ###############################\n",
        "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss, global_step=self.gstep)\n",
        "        ###############################\n",
        "\n",
        "    def create_summary(self):\n",
        "        ###############################\n",
        "        # Create summary of all losses, so we can visualize them in tensorboard.\n",
        "        with tf.name_scope('summaries'):\n",
        "            tf.summary.scalar('content loss', self.content_loss)\n",
        "            tf.summary.scalar('style loss', self.style_loss)\n",
        "            tf.summary.scalar('total loss', self.total_loss)\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "        ###############################\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        self.create_input()\n",
        "        self.load_vgg()\n",
        "        self.losses()\n",
        "        self.optimize()\n",
        "        self.create_summary()\n",
        "\n",
        "    def train(self, n_iters):\n",
        "        skip_step = 1\n",
        "        with tf.Session() as sess:\n",
        "            \n",
        "            ###############################\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            writer = tf.summary.FileWriter('graphs/style_stranfer', sess.graph)\n",
        "            ###############################\n",
        "            sess.run(self.input_img.assign(self.initial_img))\n",
        "\n",
        "            ###############################\n",
        "            ## created a saver object.\n",
        "            ## restore the variables if a checkpoint exists.\n",
        "            saver = tf.train.Saver()\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/style_transfer/checkpoint'))\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            ##############################\n",
        "\n",
        "            initial_step = self.gstep.eval()\n",
        "            \n",
        "            start_time = time.time()\n",
        "            for index in range(initial_step, n_iters):\n",
        "                if index >= 5 and index < 20:\n",
        "                    skip_step = 10\n",
        "                elif index >= 20:\n",
        "                    skip_step = 20\n",
        "                \n",
        "                sess.run(self.opt) \n",
        "                if (index + 1) % skip_step == 0:\n",
        "                    ###############################\n",
        "                    gen_image, total_loss, summary = sess.run([self.input_img,\n",
        "                                                                self.total_loss,\n",
        "                                                                self.summary_op])\n",
        "\n",
        "                    ###############################\n",
        "                    \n",
        "                    # Need to add back the mean pixels we subtracted before\n",
        "                    gen_image = gen_image + self.vgg.mean_pixels \n",
        "                    writer.add_summary(summary, global_step=index)\n",
        "                    print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
        "                    print('   Loss: {:5.1f}'.format(total_loss))\n",
        "                    print('   Took: {} seconds'.format(time.time() - start_time))\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    filename = 'outputs/%d.png' % (index)\n",
        "                    save_image(filename, gen_image)\n",
        "\n",
        "                    if (index + 1) % 20 == 0:\n",
        "                        ###############################\n",
        "                        saver.save(sess, 'checkpoints/style_stranfer/style_transfer', index)\n",
        "                        ###############################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup()\n",
        "    machine = StyleTransfer('Varagina Banakar Aravind_DACS.jpg', 'the_scream_by_edvard_munch.jpg', 333, 250)\n",
        "    machine.build()\n",
        "    machine.train(300)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-trained Object detection VGG-19 model exists already.\n",
            "INFO:tensorflow:Summary name content loss is illegal; using content_loss instead.\n",
            "INFO:tensorflow:Summary name style loss is illegal; using style_loss instead.\n",
            "INFO:tensorflow:Summary name total loss is illegal; using total_loss instead.\n",
            "Step 1\n",
            "   Sum: 48500489.3\n",
            "   Loss: 1650841600.0\n",
            "   Took: 5.005255222320557 seconds\n",
            "Step 2\n",
            "   Sum: 48500152.9\n",
            "   Loss: 1456552832.0\n",
            "   Took: 0.17206311225891113 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 3\n",
            "   Sum: 48500820.1\n",
            "   Loss: 1301022720.0\n",
            "   Took: 0.16085600852966309 seconds\n",
            "Step 4\n",
            "   Sum: 48501166.9\n",
            "   Loss: 1177465344.0\n",
            "   Took: 0.15407800674438477 seconds\n",
            "Step 5\n",
            "   Sum: 48500080.9\n",
            "   Loss: 1081616256.0\n",
            "   Took: 0.15372467041015625 seconds\n",
            "Step 10\n",
            "   Sum: 48452387.5\n",
            "   Loss: 793877568.0\n",
            "   Took: 0.5451688766479492 seconds\n",
            "Step 20\n",
            "   Sum: 48181148.5\n",
            "   Loss: 527253376.0\n",
            "   Took: 1.02260160446167 seconds\n",
            "Step 40\n",
            "   Sum: 47489079.8\n",
            "   Loss: 314573440.0\n",
            "   Took: 2.4152352809906006 seconds\n",
            "Step 60\n",
            "   Sum: 46791165.3\n",
            "   Loss: 224358688.0\n",
            "   Took: 2.853074312210083 seconds\n",
            "Step 80\n",
            "   Sum: 46124431.6\n",
            "   Loss: 175399696.0\n",
            "   Took: 2.417937755584717 seconds\n",
            "Step 100\n",
            "   Sum: 45489640.2\n",
            "   Loss: 145006096.0\n",
            "   Took: 2.8651487827301025 seconds\n",
            "Step 120\n",
            "   Sum: 44888028.5\n",
            "   Loss: 124051272.0\n",
            "   Took: 2.489882469177246 seconds\n",
            "Step 140\n",
            "   Sum: 44314851.7\n",
            "   Loss: 108262488.0\n",
            "   Took: 2.3960471153259277 seconds\n",
            "Step 160\n",
            "   Sum: 43766735.7\n",
            "   Loss: 95741416.0\n",
            "   Took: 2.5008392333984375 seconds\n",
            "Step 180\n",
            "   Sum: 43243258.1\n",
            "   Loss: 85384640.0\n",
            "   Took: 2.389735221862793 seconds\n",
            "Step 200\n",
            "   Sum: 42744328.1\n",
            "   Loss: 76646536.0\n",
            "   Took: 2.4533348083496094 seconds\n",
            "Step 220\n",
            "   Sum: 42267776.0\n",
            "   Loss: 69106464.0\n",
            "   Took: 2.489495277404785 seconds\n",
            "Step 240\n",
            "   Sum: 41812071.7\n",
            "   Loss: 62479864.0\n",
            "   Took: 2.4365594387054443 seconds\n",
            "Step 260\n",
            "   Sum: 41375824.7\n",
            "   Loss: 56700960.0\n",
            "   Took: 2.5223677158355713 seconds\n",
            "Step 280\n",
            "   Sum: 40959285.0\n",
            "   Loss: 51706440.0\n",
            "   Took: 2.4576892852783203 seconds\n",
            "Step 300\n",
            "   Sum: 40560533.0\n",
            "   Loss: 47302408.0\n",
            "   Took: 2.489532232284546 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}